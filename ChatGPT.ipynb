{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1cf305-3316-47c0-8922-69b6e8d4a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "from transformers import pipeline, set_seed, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf5b794-222b-4e80-b71c-6283479495b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2b6289-affb-446b-920a-2d1c61c77f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"datasets/NLP/companies_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cde51f64-0261-4c4b-a224-bb50e43049b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for data preprocessing\n",
    "def remove_punctuation(text):\n",
    "    punctuationFree = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationFree\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    output = [i for i in text if i not in stopWords]\n",
    "    return output\n",
    "\n",
    "def stemming(text):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "    return stem_text\n",
    "\n",
    "def lemmatizer(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa25205a-eeac-4af2-89fa-14de855770bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "# nltk.download()\n",
    "\n",
    "# Load NLTK stop words\n",
    "stopWords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "801adf85-ab2e-4b37-8651-3ffaa984871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing steps to the DataFrame\n",
    "df[\"puncFree\"] = df['description'].apply(lambda x: remove_punctuation(x))\n",
    "df[\"lowercased\"] = df[\"puncFree\"].apply(lambda x: x.lower())\n",
    "df[\"tokenized\"] = df[\"lowercased\"].apply(lambda x: tokenization(x))\n",
    "df[\"no_stop_words\"] = df[\"tokenized\"].apply(lambda x: remove_stopwords(x))\n",
    "df['stemmed'] = df['no_stop_words'].apply(lambda x: stemming(x))\n",
    "df['lemmatized'] = df['no_stop_words'].apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584a9cf9-3ccf-4f54-ad0d-a5b8eb9f991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 17:11:02.371699: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Load Keras NLP models and preprocessors\n",
    "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\"gpt2_base_en\", sequence_length=128)\n",
    "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_base_en\", preprocessor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc46ec65-725b-4999-82f7-aef3904acaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# Load Hugging Face's GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "306a5bdf-0340-439d-85ff-55ed0dd8db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenized_data_main = tokenizer([str(i) for i in df[\"lemmatized\"].values], return_tensors=\"np\", padding=True)\n",
    "tokenized_data_label = tokenizer([str(i) for i in df[\"companyName\"].values], return_tensors=\"np\", padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "622c3fcc-2949-45c8-b5f5-38f0aec15887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenized data\n",
    "tokenized_data_main = dict(tokenized_data_main)\n",
    "tokenized_data_label = dict(tokenized_data_label)\n",
    "tokenized_data_main['padding_mask'] = tokenized_data_main['attention_mask']\n",
    "del tokenized_data_main['attention_mask']\n",
    "tokenized_data_main['token_ids'] = tokenized_data_main['input_ids']\n",
    "del tokenized_data_main['input_ids']\n",
    "tokenized_data_label['padding_mask'] = tokenized_data_label['attention_mask']\n",
    "del tokenized_data_label['attention_mask']\n",
    "tokenized_data_label['token_ids'] = tokenized_data_label['input_ids']\n",
    "del tokenized_data_label['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7142137e-4340-4229-a55e-b968b1fa7fc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Compile and fit the model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m gpt2_lm\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m----> 9\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdam\u001b[49m(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[1;32m     10\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m     11\u001b[0m     weighted_metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m gpt2_lm\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, epochs\u001b[38;5;241m=\u001b[39mnum_epochs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Adam' is not defined"
     ]
    }
   ],
   "source": [
    "# Define training data\n",
    "x = tokenized_data_main\n",
    "y = tokenized_data_label\n",
    "num_epochs = 1\n",
    "\n",
    "# Compile and fit the model\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "gpt2_lm.compile(\n",
    "    optimizer = Adam(learning_rate=0.001),\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "gpt2_lm.fit(x=x, y=y, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27ae2029-5717-4046-a587-55c62e3fd358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "2023-08-18 17:15:00.760067: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [190,15]\n",
      "\t [[{{node Placeholder/_3}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  *\n        outputs = model.train_step(data)\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step  **\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 236, in __call__\n        y_true = self._conform_to_outputs(y_pred, y_true)\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 60, in _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 805, in map_to_output_names\n        raise ValueError(\n\n    ValueError: Found unexpected losses or metrics that do not correspond to any Model output: dict_keys(['padding_mask', 'token_ids']). Valid mode output names: ['reverse_embedding']. Received struct is: {'padding_mask': <tf.Tensor 'data_2:0' shape=(None, 15) dtype=int64>, 'token_ids': <tf.Tensor 'data_3:0' shape=(None, 15) dtype=int64>}.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 82\u001b[0m\n\u001b[1;32m     76\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)  \u001b[38;5;66;03m# Adjust the learning rate as needed\u001b[39;00m\n\u001b[1;32m     77\u001b[0m gpt2_lm\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     78\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     79\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m     80\u001b[0m     weighted_metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     81\u001b[0m )\n\u001b[0;32m---> 82\u001b[0m \u001b[43mgpt2_lm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras_nlp/src/utils/pipeline_model.py:202\u001b[0m, in \u001b[0;36mPipelineModel.fit\u001b[0;34m(self, x, y, batch_size, sample_weight, validation_data, validation_split, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m         (vx, vy, vsw) \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(\n\u001b[1;32m    196\u001b[0m             validation_data\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m         validation_data \u001b[38;5;241m=\u001b[39m _convert_inputs_to_dataset(\n\u001b[1;32m    199\u001b[0m             vx, vy, vsw, batch_size\n\u001b[1;32m    200\u001b[0m         )\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filefkgmqk9v.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file54v37bdp.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      9\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcontrol_dependencies(ag__\u001b[38;5;241m.\u001b[39mld(_minimum_control_deps)(ag__\u001b[38;5;241m.\u001b[39mld(outputs))):\n\u001b[1;32m     13\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39m_train_counter\u001b[38;5;241m.\u001b[39massign_add, (\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  *\n        outputs = model.train_step(data)\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step  **\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 236, in __call__\n        y_true = self._conform_to_outputs(y_pred, y_true)\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 60, in _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    File \"/home/wondermary/anaconda3/envs/NLP2/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 805, in map_to_output_names\n        raise ValueError(\n\n    ValueError: Found unexpected losses or metrics that do not correspond to any Model output: dict_keys(['padding_mask', 'token_ids']). Valid mode output names: ['reverse_embedding']. Received struct is: {'padding_mask': <tf.Tensor 'data_2:0' shape=(None, 15) dtype=int64>, 'token_ids': <tf.Tensor 'data_3:0' shape=(None, 15) dtype=int64>}.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "import keras\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"datasets/NLP/companies_data.csv\")\n",
    "\n",
    "# Define functions for data preprocessing\n",
    "def remove_punctuation(text):\n",
    "    punctuationFree = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationFree\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    output = [i for i in text if i not in stopWords]\n",
    "    return output\n",
    "\n",
    "def stemming(text):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "    return stem_text\n",
    "\n",
    "def lemmatizer(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download()\n",
    "\n",
    "# Load NLTK stop words\n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Apply preprocessing steps to the DataFrame\n",
    "df[\"puncFree\"] = df['description'].apply(lambda x: remove_punctuation(x))\n",
    "df[\"lowercased\"] = df[\"puncFree\"].apply(lambda x: x.lower())\n",
    "df[\"tokenized\"] = df[\"lowercased\"].apply(lambda x: tokenization(x))\n",
    "df[\"no_stop_words\"] = df[\"tokenized\"].apply(lambda x: remove_stopwords(x))\n",
    "df['stemmed'] = df['no_stop_words'].apply(lambda x: stemming(x))\n",
    "df['lemmatized'] = df['no_stop_words'].apply(lambda x: lemmatizer(x))\n",
    "\n",
    "# Load Keras NLP models and preprocessors\n",
    "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\"gpt2_base_en\", sequence_length=128)\n",
    "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_base_en\", preprocessor=None)\n",
    "\n",
    "# Load Hugging Face's GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_data_main = tokenizer([str(i) for i in df[\"lemmatized\"].values], return_tensors=\"np\", padding=True)\n",
    "tokenized_data_label = tokenizer([str(i) for i in df[\"companyName\"].values], return_tensors=\"np\", padding=True)\n",
    "\n",
    "# ... Rest of your code ...\n",
    "\n",
    "# Compile and fit the model\n",
    "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate=0.001)  # Adjust the learning rate as needed\n",
    "gpt2_lm.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "gpt2_lm.fit(x=x, y=y, epochs=num_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
